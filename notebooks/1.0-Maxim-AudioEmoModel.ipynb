{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468e87d8-163e-4b5d-bdd2-ff5dd0bf8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "import re\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcac7ad-a593-48ea-904b-d1c96d075e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/maxim/Programs/Projects/AGender/agender_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to /home/maxim/.cache/torch/hub/master.zip\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "from soft.models.models import AudioFeatureExtractor\n",
    "\n",
    "vad_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                                  model='silero_vad',\n",
    "                                  force_reload=True,\n",
    "                                  onnx=False)\n",
    "\n",
    "(get_speech_timestamps, _, read_audio, _, _) = utils\n",
    "\n",
    "\n",
    "def convert_video_to_audio(file_path: str, sr: int = 16000) -> str:\n",
    "    path_save = file_path.split('.')[0] + \".wav\"\n",
    "    if not os.path.exists(path_save):\n",
    "        ffmpeg_command = f\"ffmpeg -y -i {file_path} -async 1 -vn -acodec pcm_s16le -ar {sr} {path_save}\"\n",
    "        subprocess.call(ffmpeg_command, shell=True)\n",
    "\n",
    "    return path_save\n",
    "\n",
    "\n",
    "def readetect_speech(file_path: str, \n",
    "                     sr: int = 16000) -> list[dict]:\n",
    "    wav = read_audio(file_path, sampling_rate=sr)\n",
    "    # get speech timestamps from full audio file\n",
    "    speech_timestamps = get_speech_timestamps(wav, vad_model, sampling_rate=sr)\n",
    "    \n",
    "    return wav, speech_timestamps\n",
    "\n",
    "\n",
    "def find_intersections(x: list[dict], y: list[dict], min_length: float = 0) -> list[dict]:\n",
    "    \"\"\"Find intersections of two lists of dicts with intervals, preserving structure of `x` and adding intersection info.\n",
    "\n",
    "    Args:\n",
    "        x (list[dict]): First list of intervals\n",
    "        y (list[dict]): Second list of intervals\n",
    "        min_length (float, optional): Minimum length of intersection. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Windows with intersections, maintaining structure of `x`, and indicating intersection presence.\n",
    "    \"\"\"\n",
    "    timings = []\n",
    "    j = 0\n",
    "\n",
    "    for interval_x in x:\n",
    "        original_start = interval_x['start']\n",
    "        original_end = interval_x['end']\n",
    "        intersections_found = False\n",
    "\n",
    "        while j < len(y) and y[j]['end'] < original_start:\n",
    "            j += 1  # Skip any intervals in `y` that end before the current interval in `x` starts\n",
    "\n",
    "        # Check for all overlapping intervals in `y`\n",
    "        temp_j = j  # Temporary pointer to check intersections within `y` for current `x`\n",
    "        while temp_j < len(y) and y[temp_j]['start'] <= original_end:\n",
    "            # Calculate the intersection between `x[i]` and `y[j]`\n",
    "            intersection_start = max(original_start, y[temp_j]['start'])\n",
    "            intersection_end = min(original_end, y[temp_j]['end'])\n",
    "\n",
    "            if intersection_start < intersection_end and (intersection_end - intersection_start) >= min_length:\n",
    "                timings.append({\n",
    "                    'original_start': original_start,\n",
    "                    'original_end': original_end,\n",
    "                    'start': intersection_start,\n",
    "                    'end': intersection_end,\n",
    "                    'speech': True\n",
    "                })\n",
    "                intersections_found = True\n",
    "\n",
    "            temp_j += 1  # Move to the next interval in `y` for further intersections\n",
    "\n",
    "        # If no intersections were found, add the interval with `intersected` set to False\n",
    "        if not intersections_found:\n",
    "            timings.append({\n",
    "                'original_start': original_start,\n",
    "                'original_end': original_end,\n",
    "                'start': None,\n",
    "                'end': None,\n",
    "                'speech': False\n",
    "            })\n",
    "\n",
    "    return timings\n",
    "\n",
    "\n",
    "def slice_audio(start_time: float, end_time: float, \n",
    "                win_max_length: float, win_shift: float, win_min_length: float) -> list[dict]:\n",
    "    \"\"\"Slices audio on windows\n",
    "\n",
    "    Args:\n",
    "        start_time (float): Start time of audio\n",
    "        end_time (float): End time of audio\n",
    "        win_max_length (float): Window max length\n",
    "        win_shift (float): Window shift\n",
    "        win_min_length (float): Window min length\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of dict with timings, f.e.: {'start': 0, 'end': 12}\n",
    "    \"\"\"    \n",
    "\n",
    "    if end_time < start_time:\n",
    "        return []\n",
    "    elif (end_time - start_time) > win_max_length:\n",
    "        timings = []\n",
    "        while start_time < end_time:\n",
    "            end_time_chunk = start_time + win_max_length\n",
    "            if end_time_chunk < end_time:\n",
    "                timings.append({'start': start_time, 'end': end_time_chunk})\n",
    "            elif end_time_chunk == end_time: # if tail exact `win_max_length` seconds\n",
    "                timings.append({'start': start_time, 'end': end_time_chunk})\n",
    "                break\n",
    "            else: # if tail less then `win_max_length` seconds\n",
    "                if end_time - start_time < win_min_length: # if tail less then `win_min_length` seconds\n",
    "                    break\n",
    "                \n",
    "                timings.append({'start': start_time, 'end': end_time})\n",
    "                break\n",
    "\n",
    "            start_time += win_shift\n",
    "        return timings\n",
    "    else:\n",
    "        return [{'start': start_time, 'end': end_time}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb4ede5-d6a9-452a-a400-4f62052f494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/maxim/Programs/Projects/AGender/agender_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/media/maxim/Programs/Projects/AGender/agender_env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint_path = 'models/w-AudioModelWT_weights.pth'\n",
    "afe = AudioFeatureExtractor(checkpoint_path, device)\n",
    "file_path = 'example_file.mp4'\n",
    "sr = 16000\n",
    "\n",
    "new_file_path = convert_video_to_audio(file_path=file_path, sr=sr)\n",
    "wav, vad_info = readetect_speech(file_path=new_file_path, sr=sr)\n",
    "audio_windows = slice_audio(start_time=0, end_time=int(len(wav)),\n",
    "                            win_max_length=int(4 * sr), \n",
    "                            win_shift=int(2 * sr), win_min_length=int(2 * sr))\n",
    "\n",
    "intersections = find_intersections(x=audio_windows, y=vad_info, min_length=int(2 * sr))\n",
    "\n",
    "res = []\n",
    "for w_idx, window in enumerate(intersections):\n",
    "    if not window['speech']:\n",
    "        res.append({\n",
    "            'emo': None,\n",
    "            'sen': None,\n",
    "            'fea': None,\n",
    "        })\n",
    "        continue\n",
    "        \n",
    "    wave = wav[window['start']: window['end']].clone()\n",
    "    predicts, features = afe(wave)\n",
    "\n",
    "    res.append({\n",
    "        'emo': predicts['emo'], # ['neutral', 'happy', 'sad', 'anger', 'surprise', 'disgust', 'fear']\n",
    "        'sen': predicts['sen'], # ['negative', 'neutral', 'positive']\n",
    "        'fea': features,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30af4746-ed97-4ec8-b145-6161de5e33d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'emo': tensor([0.0063, 0.8051, 0.0654, 0.0577, 0.0274, 0.0291, 0.0091]),\n",
       "  'sen': tensor([0.0211, 0.9728, 0.0061]),\n",
       "  'fea': None},\n",
       " {'emo': tensor([1.2339e-07, 9.8426e-01, 1.6798e-04, 4.9909e-04, 1.4139e-02, 8.7509e-05,\n",
       "          8.4590e-04]),\n",
       "  'sen': tensor([1.5948e-08, 1.3211e-08, 1.0000e+00]),\n",
       "  'fea': None},\n",
       " {'emo': tensor([1.3111e-08, 9.9434e-01, 4.1480e-05, 3.1078e-05, 5.3832e-03, 6.9627e-06,\n",
       "          1.9564e-04]),\n",
       "  'sen': tensor([4.3964e-09, 4.1938e-09, 1.0000e+00]),\n",
       "  'fea': None},\n",
       " {'emo': tensor([7.0702e-08, 9.8921e-01, 3.9427e-05, 6.2164e-04, 9.7759e-03, 4.3741e-05,\n",
       "          3.0579e-04]),\n",
       "  'sen': tensor([3.6061e-09, 6.8011e-09, 1.0000e+00]),\n",
       "  'fea': None},\n",
       " {'emo': tensor([9.3258e-08, 9.9803e-01, 8.9725e-05, 8.4152e-05, 1.5301e-03, 4.3461e-06,\n",
       "          2.6527e-04]),\n",
       "  'sen': tensor([3.5798e-11, 2.6737e-10, 1.0000e+00]),\n",
       "  'fea': None},\n",
       " {'emo': tensor([2.3990e-06, 9.7891e-01, 1.1962e-02, 3.2124e-03, 2.4369e-03, 3.9619e-04,\n",
       "          3.0830e-03]),\n",
       "  'sen': tensor([4.3696e-09, 6.6178e-09, 1.0000e+00]),\n",
       "  'fea': None},\n",
       " {'emo': tensor([1.8047e-05, 9.9853e-01, 8.7380e-04, 4.0951e-05, 2.6046e-04, 1.2076e-06,\n",
       "          2.7114e-04]),\n",
       "  'sen': tensor([4.9379e-12, 1.9362e-09, 1.0000e+00]),\n",
       "  'fea': None},\n",
       " {'emo': tensor([6.2787e-07, 9.8554e-01, 4.1155e-03, 1.3331e-03, 6.3480e-03, 2.5575e-04,\n",
       "          2.4024e-03]),\n",
       "  'sen': tensor([7.3017e-09, 8.7651e-09, 1.0000e+00]),\n",
       "  'fea': None},\n",
       " {'emo': None, 'sen': None, 'fea': None}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
